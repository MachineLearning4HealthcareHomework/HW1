{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, accuracy_score\n",
    "from joblib import dump, load\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, Flatten,\\\n",
    "    concatenate,SimpleRNN,LSTM,Embedding,GRU,Bidirectional,Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data marshalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbdb_normal = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "ptbdb_abnormal = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "ptbdb = pd.concat([ptbdb_normal, ptbdb_abnormal])\n",
    "\n",
    "mitbih_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "mitbih_train = mitbih_train.sample(frac=1)\n",
    "mitbih_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "\n",
    "ptbdb_categories=2\n",
    "ptbdb_train, ptbdb_test = train_test_split(ptbdb, test_size=0.2, random_state=42, stratify=ptbdb[187])\n",
    "ptbdb_train_Y = np.array(ptbdb_train[187].values).astype(np.int8)\n",
    "ptbdb_train_X = np.array(ptbdb_train[list(range(187))].values)[..., np.newaxis]\n",
    "ptbdb_validation_x, ptbdb_train_x, ptbdb_validation_y, ptbdb_train_y = train_test_split(ptbdb_train_X, ptbdb_train_Y, test_size=0.33, random_state=42)\n",
    "ptbdb_test_Y = np.array(ptbdb_test[187].values).astype(np.int8)\n",
    "ptbdb_test_X = np.array(ptbdb_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "mitbih_categories=5\n",
    "mitbih_train_Y = np.array(mitbih_train[187].values).astype(np.int8)\n",
    "mitbih_train_X = np.array(mitbih_train[list(range(187))].values)[..., np.newaxis]\n",
    "mitbih_validation_x, mitbih_train_x, mitbih_validation_y, mitbih_train_y = train_test_split(mitbih_train_X, mitbih_train_Y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "|-dropout: 0.30000000000000004\n",
    "|-gate_type: GRU\n",
    "|-learning_rate: 0.01\n",
    "|-num_layers: 1\n",
    "|-optimizer: ADAM\n",
    "|-recurrent_dropout: 0.2\n",
    "|-tuner/bracket: 3\n",
    "|-tuner/epochs: 5\n",
    "|-tuner/initial_epoch: 2\n",
    "|-tuner/round: 1\n",
    "|-tuner/trial_id: 1be35566db3f7b602560eef664260300\n",
    "|-units: 137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 187, 137)          57540     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 137)               113436    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8832      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_mitbih (Dense)       (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 184,228\n",
      "Trainable params: 184,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8277\n",
      "Epoch 00001: val_acc improved from -inf to 0.81187, saving model to baseline_rnn_mitbih.h5\n",
      "78798/78798 [==============================] - 1705s 22ms/sample - loss: nan - acc: 0.8277 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 2/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8279\n",
      "Epoch 00002: val_acc did not improve from 0.81187\n",
      "78798/78798 [==============================] - 1634s 21ms/sample - loss: nan - acc: 0.8279 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 3/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8279\n",
      "Epoch 00003: val_acc did not improve from 0.81187\n",
      "78798/78798 [==============================] - 1637s 21ms/sample - loss: nan - acc: 0.8279 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 4/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8279\n",
      "Epoch 00004: val_acc did not improve from 0.81187\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "78798/78798 [==============================] - 1630s 21ms/sample - loss: nan - acc: 0.8279 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 5/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8279\n",
      "Epoch 00005: val_acc did not improve from 0.81187\n",
      "78798/78798 [==============================] - 1634s 21ms/sample - loss: nan - acc: 0.8279 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 6/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - acc: 0.8279\n",
      "Epoch 00006: val_acc did not improve from 0.81187\n",
      "78798/78798 [==============================] - 1633s 21ms/sample - loss: nan - acc: 0.8279 - val_loss: nan - val_acc: 0.8119\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d42c0965f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('hi')\n",
    "units=137\n",
    "num_layers=1\n",
    "gate_type='GRU'\n",
    "learning_rate=0.01\n",
    "dropout_rate=0.3\n",
    "optimizer='ADAM'\n",
    "recurrent_dropout_rate=0.2\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=0.,input_shape=(187, 1)))\n",
    "for i in range(num_layers):\n",
    "    if gate_type=='SimpleRNN':\n",
    "        model.add(SimpleRNN(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='GRU':\n",
    "        model.add(GRU(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='LSTM':\n",
    "        model.add(LSTM(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='Bidirectional':\n",
    "        model.add(Bidirectional(LSTM(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate)))\n",
    "if gate_type=='SimpleRNN':\n",
    "    model.add(SimpleRNN(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "elif gate_type=='GRU':\n",
    "    model.add(GRU(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "elif gate_type=='LSTM':\n",
    "    model.add(LSTM(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "elif gate_type=='Bidirectional':\n",
    "    model.add(Bidirectional(LSTM(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate)))\n",
    "model.add(Dense(64, activation=activations.relu, name=\"dense_1\"))\n",
    "model.add(Dense(64, activation=activations.relu, name=\"dense_2\"))\n",
    "model.add(Dense(4, activation=activations.softmax, name=\"dense_3_mitbih\"))\n",
    "optimizer_type=optimizer\n",
    "learning_rate=learning_rate\n",
    "if optimizer_type=='SGD':\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "elif optimizer_type=='ADAM':\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate)\n",
    "elif optimizer_type=='RMSprop':\n",
    "    optimizer=keras.optimizers.RMSprop(\n",
    "        learning_rate=learning_rate)\n",
    "loss=losses.sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['acc'])\n",
    "file_path = \"baseline_rnn_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "model.build()\n",
    "model.summary()\n",
    "model.fit(mitbih_train_X, mitbih_train_Y, epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1,validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 187, 137)          57540     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 137)               113436    \n",
      "=================================================================\n",
      "Total params: 170,976\n",
      "Trainable params: 170,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.Sequential(model.layers)#since I already popped three times, otherwise use model.layers[:-3]\n",
    "model1.build(input_shape=(187, 1))\n",
    "model1.summary()\n",
    "model1.save('transferral_base.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part of the first transferral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train_X=model1.predict(ptbdb_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train_X=np.expand_dims(predictions_train_X, axis=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part of first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 137, 64)           128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 137, 64)           4160      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8768)              0         \n",
      "_________________________________________________________________\n",
      "dense_3_ptbdb (Dense)        (None, 1)                 8769      \n",
      "=================================================================\n",
      "Total params: 13,057\n",
      "Trainable params: 13,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2=keras.Sequential()\n",
    "model2.add(Input(shape=(137,1)))\n",
    "model2.add(Dense(64, activation=activations.relu, name=\"dense_1\"))\n",
    "model2.add(Dense(64, activation=activations.relu, name=\"dense_2\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(1, activation=activations.sigmoid, name=\"dense_3_ptbdb\"))\n",
    "if optimizer_type=='SGD':\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "elif optimizer_type=='ADAM':\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate)\n",
    "elif optimizer_type=='RMSprop':\n",
    "    optimizer=keras.optimizers.RMSprop(\n",
    "        learning_rate=learning_rate)\n",
    "loss=losses.binary_crossentropy\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['acc'])\n",
    "file_path = \"transferral_model2.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "model2.build()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11641 samples\n",
      "11641/11641 [==============================] - 2s 157us/sample - loss: 0.6028 - acc: 0.7200\n"
     ]
    }
   ],
   "source": [
    "model2.fit(predictions_train_X,ptbdb_train_Y)\n",
    "model2.save('transfer2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 187, 137)          57540     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 137)               113436    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8832      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 184,033\n",
      "Trainable params: 184,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.add(Dense(64, activation=activations.relu,name='dense_10'))\n",
    "model1.add(Dense(64, activation=activations.relu,name='dense_11'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation=activations.sigmoid,name='dense_12'))\n",
    "\n",
    "model1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['acc'])\n",
    "file_path = \"model3.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "model1.build()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11641 samples\n",
      "11641/11641 [==============================] - 241s 21ms/sample - loss: 0.5952 - acc: 0.7206\n"
     ]
    }
   ],
   "source": [
    "model1.fit(ptbdb_train_X,ptbdb_train_Y)\n",
    "model1.save('transfer3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 187, 137)          57540     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 137)               113436    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8832      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 184,033\n",
      "Trainable params: 13,057\n",
      "Non-trainable params: 170,976\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4= keras.models.load_model(\"baseline_rnn_mitbih.h5\")\n",
    "model4= keras.models.Sequential(model4.layers[:3])\n",
    "model4.trainable=False\n",
    "model4.add(Dense(64, activation=activations.relu,name='dense_10'))\n",
    "model4.add(Dense(64, activation=activations.relu,name='dense_11'))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(1, activation=activations.sigmoid,name='dense_12'))\n",
    "model4.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['acc'])\n",
    "file_path = \"model4.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "model4.build()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11641 samples\n",
      "11641/11641 [==============================] - 242s 21ms/sample - loss: 0.5932 - acc: 0.7218\n"
     ]
    }
   ],
   "source": [
    "model4.fit(ptbdb_train_X,ptbdb_train_Y)\n",
    "model4.save('transfer4.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
