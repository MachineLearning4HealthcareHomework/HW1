{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate,SimpleRNN,LSTM,Embedding,GRU,Bidirectional,Masking\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data & data marshalling\n",
    "## Create first beat plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptbdb_normal = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "ptbdb_abnormal = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "ptbdb = pd.concat([ptbdb_normal, ptbdb_abnormal])\n",
    "\n",
    "mitbih_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "mitbih_train = mitbih_train.sample(frac=1)\n",
    "mitbih_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "\n",
    "ptbdb_categories=2\n",
    "ptbdb_train, ptbdb_test = train_test_split(ptbdb, test_size=0.2, random_state=42, stratify=ptbdb[187])\n",
    "ptbdb_train_Y = np.array(ptbdb_train[187].values).astype(np.int8)\n",
    "ptbdb_train_X = np.array(ptbdb_train[list(range(187))].values)[..., np.newaxis].squeeze()\n",
    "ptbdb_validation_x, ptbdb_train_x, ptbdb_validation_y, ptbdb_train_y = train_test_split(ptbdb_train_X, ptbdb_train_Y, test_size=0.33, random_state=42)\n",
    "ptbdb_test_Y = np.array(ptbdb_test[187].values).astype(np.int8)\n",
    "ptbdb_test_X = np.array(ptbdb_test[list(range(187))].values)[..., np.newaxis].squeeze()\n",
    "ptbdb_beats=range(len(ptbdb_train_x))\n",
    "ptbdb_color=['green','red']\n",
    "ptbdb_label=[\"Normal beat\", \"Abnormal beat\"]\n",
    "\n",
    "mitbih_categories=5\n",
    "mitbih_train_Y = np.array(mitbih_train[187].values).astype(np.int8)\n",
    "mitbih_train_X = np.array(mitbih_train[list(range(187))].values)[..., np.newaxis].squeeze()\n",
    "mitbih_test_Y = np.array(mitbih_test[187].values).astype(np.int8)\n",
    "mitbih_test_X = np.array(mitbih_test[list(range(187))].values)[..., np.newaxis].squeeze()\n",
    "mitbih_validation_x, mitbih_train_x, mitbih_validation_y, mitbih_train_y = train_test_split(mitbih_train_X, mitbih_train_Y, test_size=0.33, random_state=42)\n",
    "mitbih_beats=range(len(mitbih_train_x))\n",
    "mitbih_color=['green','red','black','blue','grey']\n",
    "mitbih_label=[\"Normal\", \"Supraventricular\", \"Premature\",\"Fusion\", \"Unclassifiable\"]\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    for beat in ptbdb_beats:\n",
    "        if ptbdb_train_y[beat]==category:\n",
    "            plt.plot(range(len(ptbdb_train_x[beat])),ptbdb_train_x[beat],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "            break\n",
    "plt.legend()\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('PTBDB Different beats')\n",
    "plt.savefig(\"ptbdb_dif_beats.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    for beat in mitbih_beats:\n",
    "        if mitbih_train_y[beat]==category:\n",
    "            plt.plot(range(len(mitbih_train_x[beat])),mitbih_train_x[beat],color=mitbih_color[category],label=mitbih_label[category])\n",
    "            break\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('MITBIH Different beats')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_dif_beats.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create peaks and peaks plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window=15\n",
    "def make_peaks(data):\n",
    "    peaks=np.zeros((len(data),2*window))\n",
    "    for i in range(len(data)):\n",
    "        #I started after 10 because sometimes there is a big peak at the very start\n",
    "        peak=np.argmax(data[i][10:])+10\n",
    "        peakinfo=data[i][peak-window:peak+window]\n",
    "        for j in range(len(peakinfo)):\n",
    "            peaks[i][j]=peakinfo[j]\n",
    "    return pd.DataFrame(peaks)\n",
    "\n",
    "ptbdb_peaks=make_peaks(ptbdb_train_X)\n",
    "ptbdb_cat_peaks=pd.concat([ptbdb_peaks, pd.DataFrame(ptbdb_train_Y).rename(columns={0: \"Category\"})], axis=1)\n",
    "\n",
    "mitbih_peaks=make_peaks(mitbih_train_X)\n",
    "mitbih_cat_peaks=pd.concat([mitbih_peaks, pd.DataFrame(mitbih_train_Y).rename(columns={0: \"Category\"})], axis=1)\n",
    "\n",
    "ptbdb_cat_peaks=ptbdb_cat_peaks.groupby(['Category']).mean().T\n",
    "mitbih_cat_peaks=mitbih_cat_peaks.groupby(['Category']).mean().T\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    plt.plot(range(len(ptbdb_cat_peaks)),ptbdb_cat_peaks[category],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('PTBDB Mean peaks')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_peaks.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    plt.plot(range(len(mitbih_cat_peaks)),mitbih_cat_peaks[category],color=mitbih_color[category],label=mitbih_label[category])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('MITBIH Mean peaks')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_peaks.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make distribution plot /how many of each category exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptbdb_counts=ptbdb[187].value_counts(ascending=True).sort_index()\n",
    "mitbih_counts=pd.Series(mitbih_train_Y).value_counts().sort_index()\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    plt.bar(category,ptbdb_counts[category],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB Beat distribution')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_distribution.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    plt.bar(category,mitbih_counts[category],color=mitbih_color[category],label=mitbih_label[category])\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH Beat distribution')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_distribution.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make differences and create differences histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_differences(X):\n",
    "    differences=np.zeros((len(X),len(X[1])))\n",
    "    signals=np.zeros(len(X))\n",
    "    for beat in range(len(X)):\n",
    "        for signal in range(1,len(X[1])):\n",
    "            if X[beat][signal-1]==0 and X[beat][signal]==0:\n",
    "                if (signal>=len(X[1])-2):\n",
    "                    signals[beat]=signal\n",
    "                    break\n",
    "                else:\n",
    "                    if X[beat][signal+1]==0 and X[beat][signal+2]==0:\n",
    "                        signals[beat]=signal\n",
    "                        break\n",
    "            differences[beat][signal]=X[beat][signal]-X[beat][signal-1]\n",
    "    return differences,signals\n",
    "\n",
    "ptbdb_difCat=[[],[]]\n",
    "ptbdb_differences, ptbdb_signals=make_differences(ptbdb_train_X)\n",
    "for beat in range(len(ptbdb_differences)):\n",
    "    ptbdb_difCat[ptbdb_train_Y[beat]].extend(ptbdb_differences[beat][:int(ptbdb_signals[beat]-1)])\n",
    "\n",
    "X=mitbih_train_X\n",
    "\n",
    "mitbih_difCat=[[],[],[],[],[]]\n",
    "mitbih_differences, mitbih_signals=make_differences(mitbih_train_X)\n",
    "for beat in range(len(mitbih_differences)):\n",
    "    mitbih_difCat[mitbih_train_Y[beat]].extend(mitbih_differences[beat][:int(mitbih_signals[beat]-1)])\n",
    "\n",
    "\n",
    "plt.hist(ptbdb_difCat, bins=[-1/20,-1/40,-1/80,-1/160,0,1/160,1/80,1/40,1/20], density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram inner')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_inner.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(ptbdb_difCat,bins=[-1,-1/2,-1/4,-1/10,-1/20,-1/40],density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram left')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_left.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(ptbdb_difCat,bins=[1/40,1/20,1/10,1/4,1/2,1],density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram right')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_right.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat, bins=[-1/20,-1/40,-1/80,-1/160,0,1/160,1/80,1/40,1/20], density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram inner')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_histo_inner.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat,bins=[-1,-1/2,-1/4,-1/10,-1/20,-1/40],density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram left')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_histo_left.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat,bins=[1/40,1/20,1/10,1/4,1/2,1],density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram right')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitb_histo_right.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTBDB baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 187, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 183, 16)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 179, 16)           1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 87, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 85, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 40, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 17, 256)           24832     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_ptbdb (Dense)        (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 254,641\n",
      "Trainable params: 254,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10476 samples, validate on 1165 samples\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84807, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 6s - loss: 0.5051 - acc: 0.7431 - val_loss: 0.3941 - val_acc: 0.8481\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84807 to 0.92103, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.3349 - acc: 0.8625 - val_loss: 0.2384 - val_acc: 0.9210\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.92103 to 0.93133, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.2399 - acc: 0.9061 - val_loss: 0.1908 - val_acc: 0.9313\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.93133 to 0.94335, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1995 - acc: 0.9262 - val_loss: 0.1504 - val_acc: 0.9433\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94335\n",
      "10476/10476 - 2s - loss: 0.1808 - acc: 0.9327 - val_loss: 0.2400 - val_acc: 0.8987\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.94335\n",
      "10476/10476 - 2s - loss: 0.1531 - acc: 0.9425 - val_loss: 0.2601 - val_acc: 0.8970\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.94335 to 0.96567, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1284 - acc: 0.9529 - val_loss: 0.1016 - val_acc: 0.9657\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.96567 to 0.96910, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1244 - acc: 0.9545 - val_loss: 0.1002 - val_acc: 0.9691\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.96910\n",
      "10476/10476 - 2s - loss: 0.1089 - acc: 0.9613 - val_loss: 0.0937 - val_acc: 0.9665\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.96910\n",
      "10476/10476 - 2s - loss: 0.1051 - acc: 0.9602 - val_loss: 0.1171 - val_acc: 0.9511\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.96910 to 0.97082, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1033 - acc: 0.9617 - val_loss: 0.0873 - val_acc: 0.9708\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.97082 to 0.97682, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0865 - acc: 0.9700 - val_loss: 0.0643 - val_acc: 0.9768\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.97682\n",
      "10476/10476 - 2s - loss: 0.0784 - acc: 0.9720 - val_loss: 0.0658 - val_acc: 0.9768\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.97682 to 0.97768, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0769 - acc: 0.9723 - val_loss: 0.0672 - val_acc: 0.9777\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.97768\n",
      "10476/10476 - 2s - loss: 0.0816 - acc: 0.9695 - val_loss: 0.0704 - val_acc: 0.9674\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.97768\n",
      "10476/10476 - 2s - loss: 0.0707 - acc: 0.9764 - val_loss: 0.0650 - val_acc: 0.9760\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.97768 to 0.98455, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0666 - acc: 0.9758 - val_loss: 0.0528 - val_acc: 0.9845\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98455\n",
      "10476/10476 - 2s - loss: 0.0651 - acc: 0.9774 - val_loss: 0.1208 - val_acc: 0.9554\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98455\n",
      "10476/10476 - 2s - loss: 0.0611 - acc: 0.9789 - val_loss: 0.0669 - val_acc: 0.9794\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98455\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "10476/10476 - 2s - loss: 0.0618 - acc: 0.9784 - val_loss: 0.0442 - val_acc: 0.9845\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.98455 to 0.98970, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0387 - acc: 0.9870 - val_loss: 0.0363 - val_acc: 0.9897\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.98970 to 0.99056, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0347 - val_acc: 0.9906\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.99056 to 0.99142, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0262 - acc: 0.9913 - val_loss: 0.0332 - val_acc: 0.9914\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99142\n",
      "10476/10476 - 2s - loss: 0.0253 - acc: 0.9914 - val_loss: 0.0369 - val_acc: 0.9906\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99142\n",
      "10476/10476 - 2s - loss: 0.0279 - acc: 0.9901 - val_loss: 0.0327 - val_acc: 0.9906\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99142\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "10476/10476 - 2s - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0324 - val_acc: 0.9914\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.99142\n",
      "10476/10476 - 2s - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0325 - val_acc: 0.9914\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99142\n",
      "10476/10476 - 2s - loss: 0.0209 - acc: 0.9925 - val_loss: 0.0323 - val_acc: 0.9914\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "\n",
    "df_2 = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    nclass = 1\n",
    "    inp = Input(shape=(187, 1))\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.sigmoid, name=\"dense_3_ptbdb\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "file_path = \"baseline_cnn_ptbdb.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "test_probs = pred_test\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "ptbdb_base_acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "ptbdb_base_auroc = roc_auc_score(Y_test, test_probs)\n",
    "\n",
    "precision, recall, thresh = precision_recall_curve(Y_test,test_probs)\n",
    "ptbdb_base_auprc=auc(recall,precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MITBIH baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 187, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 183, 16)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 179, 16)           1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 87, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 85, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 40, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 17, 256)           24832     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_mitbih (Dense)       (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 254,901\n",
      "Trainable params: 254,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93821, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 17s - loss: 0.3448 - acc: 0.9022 - val_loss: 0.2553 - val_acc: 0.9382\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93821 to 0.96048, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1891 - acc: 0.9490 - val_loss: 0.1452 - val_acc: 0.9605\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96048 to 0.96848, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1453 - acc: 0.9616 - val_loss: 0.1197 - val_acc: 0.9685\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96848 to 0.97510, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1266 - acc: 0.9661 - val_loss: 0.0993 - val_acc: 0.9751\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97510 to 0.97693, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1131 - acc: 0.9698 - val_loss: 0.0900 - val_acc: 0.9769\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97693\n",
      "78798/78798 - 15s - loss: 0.1036 - acc: 0.9721 - val_loss: 0.0890 - val_acc: 0.9760\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97693\n",
      "78798/78798 - 15s - loss: 0.0943 - acc: 0.9742 - val_loss: 0.0843 - val_acc: 0.9758\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97693\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "78798/78798 - 15s - loss: 0.0896 - acc: 0.9757 - val_loss: 0.0776 - val_acc: 0.9768\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.97693 to 0.98093, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0653 - acc: 0.9814 - val_loss: 0.0651 - val_acc: 0.9809\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98093\n",
      "78798/78798 - 15s - loss: 0.0598 - acc: 0.9827 - val_loss: 0.0642 - val_acc: 0.9806\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.98093 to 0.98116, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0582 - acc: 0.9830 - val_loss: 0.0632 - val_acc: 0.9812\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.98116 to 0.98138, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0560 - acc: 0.9838 - val_loss: 0.0617 - val_acc: 0.9814\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98138\n",
      "78798/78798 - 15s - loss: 0.0541 - acc: 0.9840 - val_loss: 0.0616 - val_acc: 0.9807\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.98138 to 0.98184, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0531 - acc: 0.9841 - val_loss: 0.0599 - val_acc: 0.9818\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98184 to 0.98241, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0506 - acc: 0.9847 - val_loss: 0.0587 - val_acc: 0.9824\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.98241 to 0.98275, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0500 - acc: 0.9851 - val_loss: 0.0583 - val_acc: 0.9828\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98275\n",
      "78798/78798 - 15s - loss: 0.0489 - acc: 0.9853 - val_loss: 0.0578 - val_acc: 0.9824\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98275\n",
      "78798/78798 - 15s - loss: 0.0492 - acc: 0.9851 - val_loss: 0.0604 - val_acc: 0.9826\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98275\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "78798/78798 - 15s - loss: 0.0460 - acc: 0.9857 - val_loss: 0.0575 - val_acc: 0.9828\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98275\n",
      "78798/78798 - 15s - loss: 0.0433 - acc: 0.9865 - val_loss: 0.0577 - val_acc: 0.9826\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98275\n",
      "78798/78798 - 15s - loss: 0.0425 - acc: 0.9873 - val_loss: 0.0571 - val_acc: 0.9826\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "def get_model():\n",
    "    nclass = 5\n",
    "    inp = Input(shape=(187, 1))\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax, name=\"dense_3_mitbih\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "file_path = \"baseline_cnn_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "test_probs = pred_test\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "\n",
    "mitbih_base_acc = accuracy_score(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# own RNNs\n",
    "## Hyperparameters based on results from 'Healthcare_GPU.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10476 samples, validate on 1165 samples\n",
      "Epoch 1/1000\n",
      "10476/10476 [==============================] - 224s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 2/1000\n",
      "10476/10476 [==============================] - 219s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 3/1000\n",
      "10476/10476 [==============================] - 219s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 4/1000\n",
      "10464/10476 [============================>.] - ETA: 0s - loss: 4.2597 - accuracy: 0.7207\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "10476/10476 [==============================] - 220s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 5/1000\n",
      "10476/10476 [==============================] - 219s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 6/1000\n",
      "10476/10476 [==============================] - 221s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 00006: early stopping\n",
      "Train on 10476 samples, validate on 1165 samples\n",
      "Epoch 1/1000\n",
      "10476/10476 [==============================] - 16s 2ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 2/1000\n",
      "10476/10476 [==============================] - 13s 1ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 3/1000\n",
      "10476/10476 [==============================] - 13s 1ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 4/1000\n",
      "10464/10476 [============================>.] - ETA: 0s - loss: 4.2597 - accuracy: 0.7207\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "10476/10476 [==============================] - 13s 1ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 5/1000\n",
      "10476/10476 [==============================] - 13s 1ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 6/1000\n",
      "10476/10476 [==============================] - 13s 1ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 00006: early stopping\n",
      "Train on 10476 samples, validate on 1165 samples\n",
      "Epoch 1/1000\n",
      "10476/10476 [==============================] - 223s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 2/1000\n",
      "10476/10476 [==============================] - 219s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 3/1000\n",
      "10476/10476 [==============================] - 218s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 4/1000\n",
      "10464/10476 [============================>.] - ETA: 0s - loss: 4.2597 - accuracy: 0.7207\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "10476/10476 [==============================] - 218s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 5/1000\n",
      "10476/10476 [==============================] - 219s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 6/1000\n",
      "10476/10476 [==============================] - 218s 21ms/sample - loss: 4.2577 - accuracy: 0.7208 - val_loss: 4.0839 - val_accuracy: 0.7322\n",
      "Epoch 00006: early stopping\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "78798/78798 [==============================] - 594s 8ms/sample - loss: nan - accuracy: 0.8272 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 2/1000\n",
      "78798/78798 [==============================] - 594s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 3/1000\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 4/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.8274\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 5/1000\n",
      "78798/78798 [==============================] - 592s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 6/1000\n",
      "78798/78798 [==============================] - 592s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 00006: early stopping\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "78798/78798 [==============================] - 99s 1ms/sample - loss: nan - accuracy: 0.8272 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 2/1000\n",
      "78798/78798 [==============================] - 97s 1ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 3/1000\n",
      "78798/78798 [==============================] - 97s 1ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 4/1000\n",
      "78752/78798 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.8274\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "78798/78798 [==============================] - 97s 1ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 5/1000\n",
      "78798/78798 [==============================] - 97s 1ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 6/1000\n",
      "78798/78798 [==============================] - 97s 1ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 00006: early stopping\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "78798/78798 [==============================] - 595s 8ms/sample - loss: nan - accuracy: 0.8273 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 2/1000\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 3/1000\n",
      "78798/78798 [==============================] - 594s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 4/1000\n",
      "78784/78798 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.8274\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 5/1000\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 6/1000\n",
      "78798/78798 [==============================] - 593s 8ms/sample - loss: nan - accuracy: 0.8274 - val_loss: nan - val_accuracy: 0.8303\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "def ptbdb_scores(model,X,Y):\n",
    "    pred_test = model.predict(X)\n",
    "    test_probs = pred_test\n",
    "    pred_test = np.argmax(pred_test, axis=-1)\n",
    "    acc = accuracy_score(Y, pred_test)\n",
    "\n",
    "    auroc = roc_auc_score(Y, test_probs)\n",
    "\n",
    "    precision, recall, thresh = precision_recall_curve(Y,test_probs)\n",
    "    auprc=auc(recall,precision)\n",
    "    return acc,auroc,auprc\n",
    "early = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [early, redonplat]  # early\n",
    "\n",
    "def build(input_size, num_classes, gate_type,dropout_rate,recurrent_dropout_rate,units,num_layers,optimizer_type,learning_rate):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Masking(mask_value=0.,\n",
    "                              input_shape=(input_size, 1)))\n",
    "    for i in range(num_layers):\n",
    "        if gate_type=='SimpleRNN':\n",
    "            model.add(SimpleRNN(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "        elif gate_type=='GRU':\n",
    "            model.add(GRU(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "        elif gate_type=='LSTM':\n",
    "            model.add(LSTM(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "        elif gate_type=='Bidirectional':\n",
    "            model.add(Bidirectional(LSTM(units,return_sequences=True,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate)))\n",
    "    if gate_type=='SimpleRNN':\n",
    "        model.add(SimpleRNN(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='GRU':\n",
    "        model.add(GRU(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='LSTM':\n",
    "        model.add(LSTM(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate))\n",
    "    elif gate_type=='Bidirectional':\n",
    "        model.add(Bidirectional(LSTM(units,dropout=dropout_rate,recurrent_dropout=recurrent_dropout_rate)))\n",
    "    model.add(Dense(64, activation=activations.relu, name=\"dense_1\"))\n",
    "    model.add(Dense(64, activation=activations.relu, name=\"dense_2\"))\n",
    "    model.add(Dense(num_classes-1, activation=activations.softmax, name=\"dense_3_mitbih\"))\n",
    "    if optimizer_type=='SGD':\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_type=='ADAM':\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate)\n",
    "    elif optimizer_type=='RMSprop':\n",
    "        optimizer=keras.optimizers.RMSprop(\n",
    "            learning_rate=learning_rate)\n",
    "    if num_classes >2:\n",
    "        loss=losses.sparse_categorical_crossentropy\n",
    "    else:\n",
    "        loss=losses.binary_crossentropy\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameters: PTBDB normal/diff\n",
    "# |-dropout: 0.30000000000000004\n",
    "# |-gate_type: GRU\n",
    "# |-learning_rate: 0.01\n",
    "# |-num_layers: 1\n",
    "# |-optimizer: ADAM\n",
    "# |-recurrent_dropout: 0.2\n",
    "# |-tuner/bracket: 3\n",
    "# |-tuner/epochs: 5\n",
    "# |-tuner/initial_epoch: 2\n",
    "# |-tuner/round: 1\n",
    "# |-tuner/trial_id: 1be35566db3f7b602560eef664260300\n",
    "# |-units: 137\n",
    "ptbdb_normal_model=build(187,2, 'GRU',0.3,0.2,137,1,'ADAM',0.01)\n",
    "ptbdb_diff_model=build(187,2, 'GRU',0.3,0.2,137,1,'ADAM',0.01)\n",
    "\n",
    "# Hyperparameters: MITBIH normal/diff\n",
    "# |-dropout: 0.2\n",
    "# |-gate_type: SimpleRNN\n",
    "# |-learning_rate: 0.01\n",
    "# |-num_layers: 1\n",
    "# |-optimizer: ADAM\n",
    "# |-recurrent_dropout: 0.1\n",
    "# |-tuner/bracket: 3\n",
    "# |-tuner/epochs: 2\n",
    "# |-tuner/initial_epoch: 0\n",
    "# |-tuner/round: 0\n",
    "# |-units: 168\n",
    "mitbih_normal_model=build(187,5,'SimpleRNN',0.2,0.1,168,1,'ADAM',0.01)\n",
    "mitbih_diff_model=build(187,5,'SimpleRNN',0.2,0.1,168,1,'ADAM',0.01)\n",
    "\n",
    "# Hyperparameters: PTBDB/MITBIH peaks\n",
    "# |-dropout: 0.2\n",
    "# |-gate_type: SimpleRNN\n",
    "# |-learning_rate: 0.01\n",
    "# |-num_layers: 1\n",
    "# |-optimizer: ADAM\n",
    "# |-recurrent_dropout: 0.1\n",
    "# |-tuner/bracket: 3\n",
    "# |-tuner/epochs: 2\n",
    "# |-tuner/initial_epoch: 0\n",
    "# |-tuner/round: 0\n",
    "# |-units: 24\n",
    "ptbdb_peaks_model=build(30,2,'SimpleRNN',0.2,0.1,24,1,'ADAM',0.01)\n",
    "mitbih_peaks_model=build(30,5,'SimpleRNN',0.2,0.1,24,1,'ADAM',0.01)\n",
    "\n",
    "\n",
    "ptbdb_train_peaks=np.expand_dims(make_peaks(ptbdb_train_X),2)\n",
    "ptbdb_test_peaks=np.expand_dims(make_peaks(ptbdb_test_X),2)\n",
    "mitbih_train_peaks=np.expand_dims(make_peaks(mitbih_train_X),2)\n",
    "mitbih_test_peaks=np.expand_dims(make_peaks(mitbih_test_X),2)\n",
    "\n",
    "ptbdb_train_diff=np.expand_dims(make_differences(ptbdb_train_X)[0],2)\n",
    "ptbdb_test_diff=np.expand_dims(make_differences(ptbdb_test_X)[0],2)\n",
    "mitbih_train_diff=np.expand_dims(make_differences(mitbih_train_X)[0],2)\n",
    "mitbih_test_diff=np.expand_dims(make_differences(mitbih_test_X)[0],2)\n",
    "\n",
    "ptbdb_normal_model.fit(np.expand_dims(ptbdb_train_X,2),ptbdb_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "ptbdb_peaks_model.fit(ptbdb_train_peaks,ptbdb_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "ptbdb_diff_model.fit(ptbdb_train_diff,ptbdb_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "\n",
    "mitbih_normal_model.fit(np.expand_dims(mitbih_train_X,2),mitbih_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "mitbih_peaks_model.fit(mitbih_train_peaks,mitbih_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "mitbih_diff_model.fit(mitbih_train_diff,mitbih_train_Y,epochs=1000, verbose=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "\n",
    "\n",
    "ptbdb_normal_acc,ptbdb_normal_auroc,ptbdb_normal_auprc=ptbdb_scores(ptbdb_normal_model,np.expand_dims(ptbdb_test_X,2),ptbdb_test_Y)\n",
    "ptbdb_peaks_acc,ptbdb_peaks_auroc,ptbdb_peaks_auprc=ptbdb_scores(ptbdb_peaks_model,ptbdb_test_peaks,ptbdb_test_Y)\n",
    "ptbdb_diff_acc,ptbdb_diff_auroc,ptbdb_diff_auprc=ptbdb_scores(ptbdb_diff_model,ptbdb_test_diff,ptbdb_test_Y)\n",
    "\n",
    "pred_test = mitbih_normal_model.predict(np.expand_dims(mitbih_test_X,2))\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_normal_acc = accuracy_score(mitbih_test_Y, pred_test)\n",
    "\n",
    "pred_test = mitbih_peaks_model.predict(mitbih_test_peaks)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_peaks_acc = accuracy_score(mitbih_test_Y, pred_test)\n",
    "\n",
    "pred_test = mitbih_diff_model.predict(mitbih_test_diff)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_diff_acc = accuracy_score(mitbih_test_Y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# own SVCs\n",
    "## Hyperparameters based on 'Healthcare.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator RandomizedSearchCV from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': 36, 'kernel': 'rbf', 'class_weight': None, 'C': 10}\n",
      "{'random_state': 36, 'kernel': 'rbf', 'class_weight': None, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "mitbih_clf_normal = load('mitbih_clf_normal.joblib')\n",
    "#I did not have enough time to hypertune the other svc's (the two I did took more than a day), the hyperparameters for these svc's correspond to the ones found in the hypertuned svc's.\n",
    "print(mitbih_clf_normal.best_params_)\n",
    "mitbih_clf_normal=sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "mitbih_clf_diff=sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "ptbdb_clf_normal=sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "ptbdb_clf_diff=sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "mitbih_clf_peaks = load('mitbih_clf_peaks.joblib')\n",
    "print(mitbih_clf_peaks.best_params_)\n",
    "mitbih_clf_peaks =sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "ptbdb_clf_peaks =sklearn.svm.SVC(random_state= 36, kernel='rbf', C=10,probability=True)\n",
    "\n",
    "mitbih_clf_normal.fit(mitbih_train_X,mitbih_train_Y)\n",
    "mitbih_clf_peaks.fit(mitbih_train_peaks.squeeze(), mitbih_train_Y)\n",
    "mitbih_clf_diff.fit(mitbih_train_diff.squeeze(),mitbih_train_Y)\n",
    "\n",
    "ptbdb_clf_normal.fit(ptbdb_train_X,ptbdb_train_Y)\n",
    "ptbdb_clf_peaks.fit(ptbdb_train_peaks.squeeze(),ptbdb_train_Y)\n",
    "ptbdb_clf_diff.fit(ptbdb_train_diff.squeeze(),ptbdb_train_Y)\n",
    "\n",
    "ptbdb_svc_normal_acc=ptbdb_clf_normal.score(ptbdb_test_X,ptbdb_test_Y)\n",
    "ptbdb_svc_peaks_acc=ptbdb_clf_peaks.score(ptbdb_test_peaks.squeeze(),ptbdb_test_Y)\n",
    "ptbdb_svc_diff_acc=ptbdb_clf_diff.score(ptbdb_test_diff.squeeze(),ptbdb_test_Y)\n",
    "\n",
    "test_probs_normal=ptbdb_clf_normal.predict_proba(ptbdb_test_X)[:,1]\n",
    "test_probs_peaks=ptbdb_clf_peaks.predict_proba(ptbdb_test_peaks.squeeze())[:,1]\n",
    "test_probs_diff=ptbdb_clf_diff.predict_proba(ptbdb_test_diff.squeeze())[:,1]\n",
    "\n",
    "ptbdb_svc_normal_auroc=roc_auc_score(ptbdb_test_Y, test_probs_normal)\n",
    "ptbdb_svc_peaks_auroc = roc_auc_score(ptbdb_test_Y, test_probs_peaks)\n",
    "ptbdb_svc_diff_auroc = roc_auc_score(ptbdb_test_Y, test_probs_diff)\n",
    "\n",
    "precision, recall, thresh = precision_recall_curve(ptbdb_test_Y,test_probs_normal)\n",
    "ptbdb_svc_normal_auprc=auc(recall,precision)\n",
    "precision, recall, thresh = precision_recall_curve(ptbdb_test_Y,test_probs_peaks)\n",
    "ptbdb_svc_peaks_auprc=auc(recall,precision)\n",
    "precision, recall, thresh = precision_recall_curve(ptbdb_test_Y,test_probs_diff)\n",
    "ptbdb_svc_diff_auprc=auc(recall,precision)\n",
    "\n",
    "mitbih_svc_normal_acc = mitbih_clf_normal.score(mitbih_test_X,mitbih_test_Y)\n",
    "mitbih_svc_peaks_acc = mitbih_clf_peaks.score(mitbih_test_peaks.squeeze(),mitbih_test_Y)\n",
    "mitbih_svc_diff_acc = mitbih_clf_diff.score(mitbih_test_diff.squeeze(),mitbih_test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "## imports from models created at 'Transferral_learning.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "transfer_base=tensorflow.keras.models.load_model('transferral_base.h5')\n",
    "transfer2=tensorflow.keras.models.load_model('transfer2.h5')\n",
    "transfer2_acc,transfer2_auroc,transfer2_auprc=ptbdb_scores(transfer2,np.expand_dims(transfer_base.predict(np.expand_dims(ptbdb_test_X,2)),2),ptbdb_test_Y)\n",
    "\n",
    "transfer4 = tensorflow.keras.models.load_model('transfer4.h5')\n",
    "transfer4_acc,transfer4_auroc,transfer4_auprc=ptbdb_scores(transfer4,np.expand_dims(ptbdb_test_X,2),ptbdb_test_Y)\n",
    "transfer3 = tensorflow.keras.models.load_model('transfer3.h5')\n",
    "transfer3_acc,transfer3_auroc,transfer3_auprc=ptbdb_scores(transfer3,np.expand_dims(ptbdb_test_X,2),ptbdb_test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTBDB Baseline\n",
      "Normal: 0.9893507385778083, 0.9983308027318735, 0.9992689170662377\n",
      "PTBDB RNN\n",
      "Normal: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "Peaks: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "Diff: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "PTBDB SVC\n",
      "Normal: 0.9529371350051529, 0.9786612079378166, 0.9886240971271106\n",
      "Peaks: 0.9148059086224665, 0.9516173895248388, 0.9766256606219932\n",
      "Diff: 0.9495018893850911, 0.9783965826883337, 0.9900106687736172\n",
      "PTBDB Transfer Learning\n",
      "Transfer 2: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "Transfer 3: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "Transfer 4: 0.2779113706630024, 0.5, 0.8610443146684987\n",
      "MITBIH Baseline\n",
      "Normal: 0.9834186004019734\n",
      "MITBIH RNN\n",
      "Normal: 0.8276082587246483\n",
      "Peaks: 0.8276082587246483\n",
      "Diff: 0.8276082587246483\n",
      "MITBIH SVC\n",
      "Normal: 0.9774346793349169\n",
      "Peaks: 0.9513064133016627\n",
      "Diff: 0.9766581399598027\n"
     ]
    }
   ],
   "source": [
    "print('PTBDB Baseline')\n",
    "print('Normal: '+str(ptbdb_base_acc)+', '+str(ptbdb_base_auroc)+', '+str(ptbdb_base_auprc))\n",
    "\n",
    "print('PTBDB RNN')\n",
    "print('Normal: '+str(ptbdb_normal_acc)+', '+str(ptbdb_normal_auroc)+', '+str(ptbdb_normal_auprc))\n",
    "print('Peaks: '+str(ptbdb_peaks_acc)+', '+str(ptbdb_peaks_auroc)+', '+str(ptbdb_peaks_auprc))\n",
    "print('Diff: '+str(ptbdb_diff_acc)+', '+str(ptbdb_diff_auroc)+', '+str(ptbdb_diff_auprc))\n",
    "\n",
    "print('PTBDB SVC')\n",
    "print('Normal: '+str(ptbdb_svc_normal_acc)+', '+str(ptbdb_svc_normal_auroc)+', '+str(ptbdb_svc_normal_auprc))\n",
    "print('Peaks: '+str(ptbdb_svc_peaks_acc)+', '+str(ptbdb_svc_peaks_auroc)+', '+str(ptbdb_svc_peaks_auprc))\n",
    "print('Diff: '+str(ptbdb_svc_diff_acc)+', '+str(ptbdb_svc_diff_auroc)+', '+str(ptbdb_svc_diff_auprc))\n",
    "\n",
    "print('PTBDB Transfer Learning')\n",
    "print('Transfer 2: '+str(transfer2_acc)+', '+str(transfer2_auroc)+', '+str(transfer2_auprc))\n",
    "print('Transfer 3: '+str(transfer3_acc)+', '+str(transfer3_auroc)+', '+str(transfer3_auprc))\n",
    "print('Transfer 4: '+str(transfer4_acc)+', '+str(transfer4_auroc)+', '+str(transfer4_auprc))\n",
    "\n",
    "\n",
    "print('MITBIH Baseline')\n",
    "print('Normal: '+str(mitbih_base_acc))\n",
    "\n",
    "print('MITBIH RNN')\n",
    "print('Normal: '+str(mitbih_normal_acc))\n",
    "print('Peaks: '+str(mitbih_peaks_acc))\n",
    "print('Diff: '+str(mitbih_diff_acc))\n",
    "\n",
    "print('MITBIH SVC')\n",
    "print('Normal: '+str(mitbih_svc_normal_acc))\n",
    "print('Peaks: '+str(mitbih_svc_peaks_acc))\n",
    "print('Diff: '+str(mitbih_svc_diff_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
