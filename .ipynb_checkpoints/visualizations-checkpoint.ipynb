{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate,SimpleRNN,LSTM,Embedding,GRU,Bidirectional,Masking\n",
    "#https://keras-team.github.io/keras-tuner/examples/helloworld/\n",
    "\n",
    "from kerastuner.tuners import Hyperband\n",
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptbdb_normal = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "ptbdb_abnormal = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "ptbdb = pd.concat([ptbdb_normal, ptbdb_abnormal])\n",
    "\n",
    "mitbih_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "mitbih_train = mitbih_train.sample(frac=1)\n",
    "mitbih_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "\n",
    "ptbdb_categories=2\n",
    "ptbdb_train, ptbdb_test = train_test_split(ptbdb, test_size=0.2, random_state=42, stratify=ptbdb[187])\n",
    "ptbdb_train_Y = np.array(ptbdb_train[187].values).astype(np.int8)\n",
    "ptbdb_train_X = np.array(ptbdb_train[list(range(187))].values)[..., np.newaxis]\n",
    "ptbdb_validation_x, ptbdb_train_x, ptbdb_validation_y, ptbdb_train_y = train_test_split(ptbdb_train_X, ptbdb_train_Y, test_size=0.33, random_state=42)\n",
    "ptbdb_test_Y = np.array(ptbdb_test[187].values).astype(np.int8)\n",
    "ptbdb_test_X = np.array(ptbdb_test[list(range(187))].values)[..., np.newaxis]\n",
    "ptbdb_beats=range(len(ptbdb_train_x))\n",
    "ptbdb_color=['green','red']\n",
    "ptbdb_label=[\"Normal beat\", \"Abnormal beat\"]\n",
    "\n",
    "mitbih_categories=5\n",
    "mitbih_train_Y = np.array(mitbih_train[187].values).astype(np.int8)\n",
    "mitbih_train_X = np.array(mitbih_train[list(range(187))].values)[..., np.newaxis]\n",
    "mitbih_validation_x, mitbih_train_x, mitbih_validation_y, mitbih_train_y = train_test_split(mitbih_train_X, mitbih_train_Y, test_size=0.33, random_state=42)\n",
    "mitbih_beats=range(len(mitbih_train_x))\n",
    "mitbih_color=['green','red','black','blue','grey']\n",
    "mitbih_label=[\"Normal\", \"Supraventricular\", \"Premature\",\"Fusion\", \"Unclassifiable\"]\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    for beat in ptbdb_beats:\n",
    "        if ptbdb_train_y[beat]==category:\n",
    "            plt.plot(range(len(ptbdb_train_x[beat])),ptbdb_train_x[beat],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "            break\n",
    "plt.legend()\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('PTBDB Different beats')\n",
    "plt.savefig(\"ptbdb_dif_beats.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    for beat in mitbih_beats:\n",
    "        if mitbih_train_y[beat]==category:\n",
    "            plt.plot(range(len(mitbih_train_x[beat])),mitbih_train_x[beat],color=mitbih_color[category],label=mitbih_label[category])\n",
    "            break\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('MITBIH Different beats')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_dif_beats.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window=15\n",
    "def make_peaks(data):\n",
    "    peaks=np.zeros((len(data),2*window))\n",
    "    for i in range(len(data)):\n",
    "        #I started after 10 because sometimes there is a big peak at the very start\n",
    "        peak=np.argmax(data[i][10:])+10\n",
    "        peakinfo=data[i][peak-window:peak+window]\n",
    "        for j in range(len(peakinfo)):\n",
    "            peaks[i][j]=peakinfo[j]\n",
    "    return pd.DataFrame(peaks)\n",
    "\n",
    "ptbdb_peaks=make_peaks(ptbdb_train_X)\n",
    "ptbdb_cat_peaks=pd.concat([ptbdb_peaks, pd.DataFrame(ptbdb_train_Y).rename(columns={0: \"Category\"})], axis=1)\n",
    "\n",
    "mitbih_peaks=make_peaks(mitbih_train_X)\n",
    "mitbih_cat_peaks=pd.concat([mitbih_peaks, pd.DataFrame(mitbih_train_Y).rename(columns={0: \"Category\"})], axis=1)\n",
    "\n",
    "ptbdb_cat_peaks=ptbdb_cat_peaks.groupby(['Category']).mean().T\n",
    "mitbih_cat_peaks=mitbih_cat_peaks.groupby(['Category']).mean().T\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    plt.plot(range(len(ptbdb_cat_peaks)),ptbdb_cat_peaks[category],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('PTBDB Mean peaks')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_peaks.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    plt.plot(range(len(mitbih_cat_peaks)),mitbih_cat_peaks[category],color=mitbih_color[category],label=mitbih_label[category])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title('MITBIH Mean peaks')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_peaks.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptbdb_counts=ptbdb[187].value_counts(ascending=True).sort_index()\n",
    "mitbih_counts=pd.Series(mitbih_train_Y).value_counts().sort_index()\n",
    "\n",
    "for category in range(ptbdb_categories):\n",
    "    plt.bar(category,ptbdb_counts[category],color=ptbdb_color[category],label=ptbdb_label[category])\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB Beat distribution')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_distribution.png\")\n",
    "plt.clf()\n",
    "\n",
    "for category in range(mitbih_categories):\n",
    "    plt.bar(category,mitbih_counts[category],color=mitbih_color[category],label=mitbih_label[category])\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH Beat distribution')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_distribution.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_differences(X):\n",
    "    differences=np.zeros((len(X),len(X[1])))\n",
    "    signals=np.zeros(len(X))\n",
    "    for beat in range(len(X)):\n",
    "        for signal in range(1,len(X[1])):\n",
    "            if X[beat][signal-1]==0 and X[beat][signal]==0:\n",
    "                if (signal>=len(X[1])-2):\n",
    "                    signals[beat]=signal\n",
    "                    break\n",
    "                else:\n",
    "                    if X[beat][signal+1]==0 and X[beat][signal+2]==0:\n",
    "                        signals[beat]=signal\n",
    "                        break\n",
    "            differences[beat][signal]=X[beat][signal]-X[beat][signal-1]\n",
    "    return differences,signals\n",
    "        \n",
    "ptbdb_difCat=[[],[]]\n",
    "ptbdb_differences, ptbdb_signals=make_differences(ptbdb_train_X)\n",
    "for beat in range(len(X)):\n",
    "    ptbdb_difCat[ptbdb_train_Y[beat]].extend(ptbdb_differences[beat][:int(ptbdb_signals[beat]-1)])\n",
    "\n",
    "X=mitbih_train_X\n",
    "\n",
    "mitbih_difCat=[[],[],[],[],[]]\n",
    "mitbih_differences, mitbih_signals=make_differences(mitbih_train_X)\n",
    "for beat in range(len(X)):\n",
    "    mitbih_difCat[mitbih_train_Y[beat]].extend(mitbih_differences[beat][:int(mitbih_signals[beat]-1)])\n",
    "\n",
    "\n",
    "plt.hist(ptbdb_difCat, bins=[-1/20,-1/40,-1/80,-1/160,0,1/160,1/80,1/40,1/20], density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram inner')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_inner.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(ptbdb_difCat,bins=[-1,-1/2,-1/4,-1/10,-1/20,-1/40],density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram left')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_left.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(ptbdb_difCat,bins=[1/40,1/20,1/10,1/4,1/2,1],density=True,color=ptbdb_color, label=ptbdb_label)\n",
    "plt.xticks([])\n",
    "plt.title('PTBDB difference histogram right')\n",
    "plt.legend()\n",
    "plt.savefig(\"ptbdb_histo_right.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat, bins=[-1/20,-1/40,-1/80,-1/160,0,1/160,1/80,1/40,1/20], density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram inner')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_histo_inner.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat,bins=[-1,-1/2,-1/4,-1/10,-1/20,-1/40],density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram left')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitbih_histo_left.png\")\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(mitbih_difCat,bins=[1/40,1/20,1/10,1/4,1/2,1],density=True,color=mitbih_color, label=mitbih_label)\n",
    "plt.xticks([])\n",
    "plt.title('MITBIH difference histogram right')\n",
    "plt.legend()\n",
    "plt.savefig(\"mitb_histo_right.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 187, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 183, 16)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 179, 16)           1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 87, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 85, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 40, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 17, 256)           24832     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_ptbdb (Dense)        (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 254,641\n",
      "Trainable params: 254,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10476 samples, validate on 1165 samples\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83433, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 7s - loss: 0.5152 - acc: 0.7426 - val_loss: 0.4119 - val_acc: 0.8343\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83433 to 0.86438, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.3508 - acc: 0.8533 - val_loss: 0.3208 - val_acc: 0.8644\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86438 to 0.88326, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.2988 - acc: 0.8736 - val_loss: 0.2838 - val_acc: 0.8833\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.88326 to 0.89957, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.2538 - acc: 0.9003 - val_loss: 0.2786 - val_acc: 0.8996\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.89957 to 0.93562, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.2202 - acc: 0.9162 - val_loss: 0.1695 - val_acc: 0.9356\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93562\n",
      "10476/10476 - 2s - loss: 0.1937 - acc: 0.9256 - val_loss: 0.1722 - val_acc: 0.9330\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.93562 to 0.93820, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1787 - acc: 0.9323 - val_loss: 0.1501 - val_acc: 0.9382\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.93820 to 0.95107, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1618 - acc: 0.9396 - val_loss: 0.1116 - val_acc: 0.9511\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.95107\n",
      "10476/10476 - 2s - loss: 0.1396 - acc: 0.9449 - val_loss: 0.1256 - val_acc: 0.9502\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.95107 to 0.95966, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1320 - acc: 0.9497 - val_loss: 0.1051 - val_acc: 0.9597\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.95966 to 0.96652, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.1125 - acc: 0.9577 - val_loss: 0.0902 - val_acc: 0.9665\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.96652\n",
      "10476/10476 - 2s - loss: 0.1084 - acc: 0.9586 - val_loss: 0.1077 - val_acc: 0.9554\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.96652 to 0.96996, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0962 - acc: 0.9655 - val_loss: 0.0960 - val_acc: 0.9700\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.96996\n",
      "10476/10476 - 2s - loss: 0.1002 - acc: 0.9632 - val_loss: 0.1206 - val_acc: 0.9571\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.96996 to 0.97682, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0894 - acc: 0.9677 - val_loss: 0.0675 - val_acc: 0.9768\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.97682\n",
      "10476/10476 - 2s - loss: 0.0854 - acc: 0.9684 - val_loss: 0.0690 - val_acc: 0.9751\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.97682\n",
      "10476/10476 - 2s - loss: 0.0766 - acc: 0.9711 - val_loss: 0.1378 - val_acc: 0.9502\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.97682 to 0.97768, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0731 - acc: 0.9725 - val_loss: 0.0742 - val_acc: 0.9777\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97768 to 0.98627, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0784 - acc: 0.9694 - val_loss: 0.0421 - val_acc: 0.9863\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.98627 to 0.98798, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0721 - acc: 0.9746 - val_loss: 0.0473 - val_acc: 0.9880\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98798\n",
      "10476/10476 - 2s - loss: 0.0655 - acc: 0.9760 - val_loss: 0.0514 - val_acc: 0.9820\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.98798\n",
      "10476/10476 - 2s - loss: 0.0565 - acc: 0.9802 - val_loss: 0.0457 - val_acc: 0.9803\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98798\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "10476/10476 - 2s - loss: 0.0544 - acc: 0.9807 - val_loss: 0.0455 - val_acc: 0.9820\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.98798 to 0.98970, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0344 - acc: 0.9883 - val_loss: 0.0367 - val_acc: 0.9897\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.98970 to 0.99056, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0293 - acc: 0.9900 - val_loss: 0.0322 - val_acc: 0.9906\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99056\n",
      "10476/10476 - 2s - loss: 0.0254 - acc: 0.9913 - val_loss: 0.0302 - val_acc: 0.9897\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.99056 to 0.99227, saving model to baseline_cnn_ptbdb.h5\n",
      "10476/10476 - 2s - loss: 0.0257 - acc: 0.9903 - val_loss: 0.0238 - val_acc: 0.9923\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99227\n",
      "10476/10476 - 2s - loss: 0.0225 - acc: 0.9917 - val_loss: 0.0290 - val_acc: 0.9914\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.99227\n",
      "10476/10476 - 2s - loss: 0.0233 - acc: 0.9917 - val_loss: 0.0257 - val_acc: 0.9923\n",
      "Epoch 30/1000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00030: val_acc did not improve from 0.99227\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "10476/10476 - 2s - loss: 0.0238 - acc: 0.9916 - val_loss: 0.0304 - val_acc: 0.9923\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.99227\n",
      "10476/10476 - 2s - loss: 0.0205 - acc: 0.9930 - val_loss: 0.0278 - val_acc: 0.9923\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.99227\n",
      "10476/10476 - 2s - loss: 0.0214 - acc: 0.9919 - val_loss: 0.0268 - val_acc: 0.9914\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-499b56e57c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test f1 score : %s \"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"ptbdb_normal.csv\", header=None)\n",
    "\n",
    "df_2 = pd.read_csv(\"ptbdb_abnormal.csv\", header=None)\n",
    "\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    nclass = 1\n",
    "    inp = Input(shape=(187, 1))\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.sigmoid, name=\"dense_3_ptbdb\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "file_path = \"baseline_cnn_ptbdb.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "test_probs = pred_test\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "ptbdb_base_acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "ptbdb_base_auroc = roc_auc_score(Y_test, test_probs)\n",
    "\n",
    "precision, recall, thresh = precision_recall_curve(Y_test,test_probs)\n",
    "ptbdb_base_auprc=auc(recall,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 187, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 183, 16)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 179, 16)           1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 87, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 85, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 40, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 17, 256)           24832     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_mitbih (Dense)       (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 254,901\n",
      "Trainable params: 254,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92325, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 17s - loss: 0.3950 - acc: 0.8862 - val_loss: 0.3126 - val_acc: 0.9233\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.92325 to 0.95603, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.2287 - acc: 0.9353 - val_loss: 0.1593 - val_acc: 0.9560\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95603 to 0.96368, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1644 - acc: 0.9556 - val_loss: 0.1375 - val_acc: 0.9637\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96368 to 0.96871, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 14s - loss: 0.1376 - acc: 0.9626 - val_loss: 0.1292 - val_acc: 0.9687\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96871 to 0.97316, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1225 - acc: 0.9664 - val_loss: 0.0991 - val_acc: 0.9732\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97316\n",
      "78798/78798 - 14s - loss: 0.1124 - acc: 0.9683 - val_loss: 0.0972 - val_acc: 0.9718\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.97316 to 0.97739, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.1029 - acc: 0.9711 - val_loss: 0.0827 - val_acc: 0.9774\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97739 to 0.97887, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 14s - loss: 0.0965 - acc: 0.9726 - val_loss: 0.0750 - val_acc: 0.9789\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97887\n",
      "78798/78798 - 14s - loss: 0.0897 - acc: 0.9750 - val_loss: 0.0922 - val_acc: 0.9743\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97887\n",
      "78798/78798 - 14s - loss: 0.0818 - acc: 0.9758 - val_loss: 0.0829 - val_acc: 0.9758\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.97887\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "78798/78798 - 14s - loss: 0.0789 - acc: 0.9770 - val_loss: 0.0778 - val_acc: 0.9766\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.97887 to 0.98527, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 14s - loss: 0.0593 - acc: 0.9823 - val_loss: 0.0563 - val_acc: 0.9853\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98527\n",
      "78798/78798 - 14s - loss: 0.0538 - acc: 0.9842 - val_loss: 0.0543 - val_acc: 0.9847\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98527\n",
      "78798/78798 - 14s - loss: 0.0514 - acc: 0.9849 - val_loss: 0.0542 - val_acc: 0.9853\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98527 to 0.98561, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 14s - loss: 0.0488 - acc: 0.9854 - val_loss: 0.0515 - val_acc: 0.9856\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.98561 to 0.98584, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 14s - loss: 0.0485 - acc: 0.9856 - val_loss: 0.0512 - val_acc: 0.9858\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.98584 to 0.98641, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0462 - acc: 0.9859 - val_loss: 0.0517 - val_acc: 0.9864\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98641\n",
      "78798/78798 - 14s - loss: 0.0449 - acc: 0.9862 - val_loss: 0.0500 - val_acc: 0.9862\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98641\n",
      "78798/78798 - 14s - loss: 0.0439 - acc: 0.9868 - val_loss: 0.0533 - val_acc: 0.9854\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98641\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "78798/78798 - 14s - loss: 0.0438 - acc: 0.9868 - val_loss: 0.0498 - val_acc: 0.9858\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.98641 to 0.98652, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0410 - acc: 0.9875 - val_loss: 0.0498 - val_acc: 0.9865\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.98652 to 0.98687, saving model to baseline_cnn_mitbih.h5\n",
      "78798/78798 - 15s - loss: 0.0411 - acc: 0.9873 - val_loss: 0.0497 - val_acc: 0.9869\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98687\n",
      "78798/78798 - 15s - loss: 0.0398 - acc: 0.9877 - val_loss: 0.0499 - val_acc: 0.9864\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98687\n",
      "78798/78798 - 14s - loss: 0.0392 - acc: 0.9879 - val_loss: 0.0495 - val_acc: 0.9868\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98687\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "78798/78798 - 15s - loss: 0.0398 - acc: 0.9877 - val_loss: 0.0497 - val_acc: 0.9866\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.98687\n",
      "78798/78798 - 15s - loss: 0.0406 - acc: 0.9876 - val_loss: 0.0497 - val_acc: 0.9863\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98687\n",
      "78798/78798 - 15s - loss: 0.0389 - acc: 0.9881 - val_loss: 0.0497 - val_acc: 0.9865\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8faf9e3420a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#IMPORT libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mmitbih_base_auroc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUROC: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmitbih_base_auroc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    379\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m    383\u001b[0m                                          multi_class, average, sample_weight)\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "def get_model():\n",
    "    nclass = 5\n",
    "    inp = Input(shape=(187, 1))\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax, name=\"dense_3_mitbih\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "file_path = \"baseline_cnn_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "test_probs = pred_test\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "\n",
    "mitbih_base_acc = accuracy_score(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed: 119.4min\n",
      "[Parallel(n_jobs=-2)]: Done 120 out of 120 | elapsed: 177.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-2,\n",
       "             param_grid={'C': [0.1, 1, 10], 'class_weight': [None, 'balanced'],\n",
       "                         'kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
       "                         'random_state': [36]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[0.1, 1, 10], 'class_weight':[None, 'balanced'],'random_state':[36]}\n",
    "svc = svm.SVC()\n",
    "ptbdb_clf_normal = RandomizedSearchCV(svc, parameters,n_jobs=-2, verbose=4, random_state=36)\n",
    "ptbdb_clf_peaks = RandomizedSearchCV(svc, parameters,n_jobs=-2, verbose=4, random_state=36)\n",
    "ptbdb_clf_normal.fit(ptbdb_train_X,ptbdb_train_Y)\n",
    "ptbdb_clf_peaks.fit(ptbdb_peaks,ptbdb_train_Y)\n",
    "ptbdb_test_peaks=make_peaks(ptbdb_test_X)\n",
    "ptbdb_svc_normal_acc=ptbdb_clf_normal.score(ptbdb_test_X,ptbdb_test_Y)\n",
    "ptbdb_svc_peaks_acc=ptbdb_clf_peaks.score(ptbdb_test_peaks,ptbdb_test_Y)\n",
    "\n",
    "test_probs_normal=ptbdb_clf_normal.predict_proba(ptbdb_test_X)\n",
    "test_probs_peaks=ptbdb_clf_peaks.predict_proba(ptbdb_test_peaks)\n",
    "ptbdb_svc_normal_auroc=roc_auc_score(ptbdb_test_Y, test_probs_normal)\n",
    "ptbdb_svc_peaks_auroc = roc_auc_score(ptbdb_test_Y, test_probs_peaks)\n",
    "\n",
    "precision, recall, thresh = precision_recall_curve(ptbdb_test_Y,test_probs_normal)\n",
    "ptbdb_svc_peaks_auprc=auc(recall,precision)\n",
    "precision, recall, thresh = precision_recall_curve(ptbdb_test_Y,test_probs_peaks)\n",
    "ptbdb_svc_peaks_auprc=auc(recall,precision)\n",
    "\n",
    "mitbih_clf_normal=RandomizedSearchCV(svc, parameters,n_jobs=-2, verbose=4, random_state=36)\n",
    "mitbih_clf_peaks = RandomizedSearchCV(svc, parameters,n_jobs=-2, verbose=4, random_state=36)\n",
    "mitbih_clf_normal.fit(mitbih_train_X, mitbih_train_Y)\n",
    "mitbih_clf_peaks.fit(mitbih_peaks,mitbih_train_Y)\n",
    "mitbih_svc_normal_acc = mitbih_clf_normal.score(mitbih_test_X,mitbih_test_Y)\n",
    "mitbih_svc_peaks_acc = mitbih_clf_peaks.score(make_peaks(mitbih_test_X),mitbih_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ptbdb_normal\\untitled_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Oracle from existing project ptbdb_normal\\untitled_project\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 0/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 1/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 2/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 3/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 4/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-49-29a14fa08669>\", line 11, in build\n",
      "    model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 5/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-29a14fa08669>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-29a14fa08669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m36\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     directory='ptbdb_normal')\n\u001b[0m\u001b[0;32m     58\u001b[0m ptbdb_diff_tuner=Hyperband(\n\u001b[0;32m     59\u001b[0m     \u001b[0mMyHyperModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mptbdb_diff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\tuners\\hyperband.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0moracle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[0mhypermodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m                  **kwargs):\n\u001b[0;32m     57\u001b[0m         super(MultiExecutionTuner, self).__init__(\n\u001b[1;32m---> 58\u001b[1;33m             oracle, hypermodel, **kwargs)\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite)\u001b[0m\n\u001b[0;32m    101\u001b[0m                                     \u001b[0mproject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                                     \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                                     overwrite=overwrite)\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_populate_initial_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m_populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m    105\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# to the search space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_fail_streak\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     raise RuntimeError(\n\u001b[1;32m--> 115\u001b[1;33m                         'Too many failed attempts to build model.')\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "def ptbdb_scores(model,X,Y):\n",
    "    pred_test = model.predict(X)\n",
    "    test_probs = pred_test\n",
    "    pred_test = np.argmax(pred_test, axis=-1)\n",
    "    acc = accuracy_score(Y, pred_test)\n",
    "\n",
    "    auroc = roc_auc_score(Y, test_probs)\n",
    "\n",
    "    precision, recall, thresh = precision_recall_curve(Y,test_probs)\n",
    "    auprc=auc(recall,precision)\n",
    "    return acc,auroc,auprc    \n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        gate_type = hp.Choice('gate_type', ['SimpleRNN', 'GRU','LSTM', 'Bidirectional'])\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(Embedding(self.input_size[0], self.input_size[1],mask_zero=True))\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 1, 5)):\n",
    "            if gate_type=='SimpleRNN':\n",
    "                model.add(SimpleRNN(hp.Int('units', 12, 187, 25),return_sequences=True))\n",
    "            elif gate_type=='GRU':\n",
    "                model.add(GRU(hp.Int('units', 12, 187, 25),return_sequences=True))\n",
    "            elif gate_type=='LSTM':\n",
    "                model.add(LSTM(hp.Int('units', 12, 187, 25),return_sequences=True))\n",
    "            elif gate_type=='Bidirectional':\n",
    "                model.add(Bidirectional(LSTM(hp.Int('units', 12, 187, 25),return_squences=True)))\n",
    "            model.add(Dropout(rate=hp.Float(\n",
    "                'dropout',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.1,\n",
    "            )))\n",
    "        model.add(Dense(64, activation=activations.relu, name=\"dense_1\"))\n",
    "        model.add(Dense(64, activation=activations.relu, name=\"dense_2\"))\n",
    "        model.add(Dense(self.num_classes, activation=activations.softmax, name=\"dense_3_mitbih\"))\n",
    "        optimizer_type=hp.Choice('optimizer',['SGD', 'RMSprop', 'ADAM'])\n",
    "        if optimizer_type=='SGD':\n",
    "            optimizer=keras.optimizers.SGD(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4]))\n",
    "        elif optimizer_type=='Adam':\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4]))\n",
    "        elif optimizer_type=='RMSprop':\n",
    "            optimizer=keras.optimizers.RMSprop(\n",
    "                hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4]))\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "ptbdb_diff, trash=make_differences(ptbdb_train_X)\n",
    "ptbdb_normal_tuner = Hyperband(\n",
    "    MyHyperModel(input_size=ptbdb_train_X[0].shape, num_classes=2),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='ptbdb_normal')\n",
    "ptbdb_peaks_tuner = Hyperband(\n",
    "    MyHyperModel(input_size=ptbdb_peaks[0].shape, num_classes=2),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='ptbdb_peaks')\n",
    "ptbdb_diff_tuner=Hyperband(\n",
    "    MyHyperModel(input_size=ptbdb_diff[0].shape, num_classes=2),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='ptbdb_diff')\n",
    "\n",
    "ptbdb_normal_tuner.search(ptbdb_train_X,\n",
    "             y=ptbdb_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "ptbdb_peaks_tuner.search(ptbdb_peaks,\n",
    "             y=ptbdb_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "ptbdb_diff_tuner.search(ptbdb_diff,\n",
    "             y=ptbdb_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "ptbdb_test_diff, trash=make_differences(ptbdb_test_X)\n",
    "ptbdb_normal_acc,ptbdb_normal_auroc,ptbdb_normal_auprc=ptbdb_scores(ptbdb_normal_tuner.get_best_models(num_models=1)[0],ptbdb_test_X,ptbdb_test_Y)\n",
    "ptbdb_peaks_acc,ptbdb_peaks_auroc,ptbdb_peaks_auprc=ptbdb_scores(ptbdb_peaks_tuner.get_best_models(num_models=1)[0],make_peaks(ptbdb_test_X),ptbdb_test_Y)\n",
    "ptbdb_diff_acc,ptbdb_diff_auroc,ptbdb_diff_auprc=ptbdb_scores(ptbdb_diff_tuner.get_best_models(num_models=1)[0],ptbdb_test_diff,ptbdb_test_Y)\n",
    "\n",
    "\n",
    "mitbih_diff, trash=make_differences(mitbih_test_X)               \n",
    "mitbih_normal_tuner = Hyperband(\n",
    "    MyHyperModel(input_size=mitbih_train_X[0].shape, num_classes=5),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='mitbih_normal')\n",
    "mitbih_peaks_tuner = Hyperband(\n",
    "    MyHyperModel(input_size=mitbih_peaks[0].shape, num_classes=5),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='mitbih_peaks')\n",
    "mitbih_diff_tuner = Hyperband(\n",
    "    MyHyperModel(input_size=mitbih_diff[0].shape, num_classes=5),\n",
    "    objective='val_accuracy', seed=36,\n",
    "    max_epochs=40,\n",
    "    directory='mitbih_diff')\n",
    "mitbih_normal_tuner.search(mitbih_train_X,\n",
    "             y=mitbih_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "mitbih_peaks_tuner.search(mitbih_peaks,\n",
    "             y=mitbih_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "mitbih_diff_tuner.search(mitbih_diff,\n",
    "             y=mitbih_train_Y,\n",
    "             epochs=5,\n",
    "             validation_split=0.2)\n",
    "mitbih_normal_model=mitbih_normal_tuner.get_best_models(num_models=1)[0]\n",
    "mitbih_peaks_model=mitbih_peaks_tuner.get_best_models(num_models=1)[0]\n",
    "mitbih_diff_model=mitbih_diff_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "pred_test = mitbih_normal_model.predict(mitbih_test_X)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_normal_acc = accuracy_score(mitbih_test_Y, pred_test)\n",
    "\n",
    "pred_test = mitbih_peaks_model.predict(make_peaks(mitbih_test_X))\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_peaks_acc = accuracy_score(mitbih_test_Y, pred_test)\n",
    "mitbih_test_diff, trash =make_differences(mitbih_test_X)\n",
    "pred_test = mitbih_diff_model.predict(mitbih_test_diff)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "mitbih_diff_acc = accuracy_score(mitbih_test_Y, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on dimensions at start!!!!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
